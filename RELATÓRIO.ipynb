{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centro Federal de Educação Tecnológica de Minas Gerais\n",
    "##### Recuperação de Informação\n",
    "## Coletor Vinigorbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor: Daniel Hasan  \n",
    "Alunos: Igor Lacerda Tomich e Vinícius França Lima Vilaça  \n",
    "Descritor do Coletor:  [https://crawlercefetmgvinigor.wordpress.com/](https://crawlercefetmgvinigor.wordpress.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução  \n",
    "\n",
    "\n",
    "A coleta de dados web, ou raspagem web, é uma forma de mineração que permite a extração de dados de sites da web convertendo-os em informação estruturada para posterior análise. Para a recuperação de informação são utilizadas aplicações que realizam a coleta de forma organizada, com definição clara sobre quais informações serão coletadas, assim como, de que forma as mesmas serão armazenadas para que o usuário possa utilizá-las. Essa aplicações de coleta de dados são comumente chamados de coletores ou, em inglês, crawlers. Este relatório irá discutir os principais desafios, as decisões de arquitetura e a implementação de um crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principais desafios\n",
    "O crawler é um algoritmo que pode ter uma estrutura paralelizável, nesta implementação a fizemos desta forma e portanto aproveitamos essa possibilidade de paralelização, assim, foi possível obter uma melhora na performance do algoritmo, isso significa que ele foi capaz de coletar uma mesma quantidade de página em menor tempo quando utilizado mais de uma thread. Apesar das vantagens da paralelização é necessário que se tome os devidos cuidados, pois na estrutura utilizada, as threads compartilham algumas variáveis e por isso foi um desafio mantê-las síncronas de modo que continuasse existindo condição de corrida, ou seja, nenhuma thread ficasse totalmente bloqueada e que também não ocorresse incongruência nos dados ou que erros inesperados ocorressem.\n",
    "\tAinda foi preciso que as funções ou “módulos” (atividades), fossem implementados da maneira mais correta possível, de forma que não existisse desvios da implementação requerida, isso significa que todos os testes unitário não puderam falhar.\n",
    "\tAmbos os integrantes do grupo também não possuíam muito contato com a linguagem de implementação Python e por isso tivemos um retardo na implementação com problemas relacionados a sintaxe.\n",
    "\tPor fim, ainda foi preciso realizar uma refatoração de uma das implementações(get_next_url)  de modo que esta não mais bloqueasse as outras threads de executarem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisões de arquitetura\n",
    "A linguagem utilizada foi o Python 3 aliado ao uso do Jupyter Notebook e a IDE Visual Studio Code. Para a execução do trabalho foram implementadas as atividades propostas no jupyter notebook através do código disponibilizado pelo professor da disciplina. Além disto algumas bibliotecas essenciais para o funcionamento do algoritmo foram utilizadas.  \n",
    "\n",
    "- Beautiful Soup:  Usado para a recuperação de dados em conteúdos HTML por meio de seu parser.  \n",
    "- Threads: Usado para a paralelização de métodos.  \n",
    "- Robotparser: Usado para checar informações relacionadas ao robots.txt de um Web Site, como por exemplo, se é possível realizar o fetch da página.  \n",
    "- Urlparse: Este módulo é usado para definir uma interface de acesso a uma URL, de modo que a URL seja quebrada ou remontada dentre seus componentes (addressing scheme, network location, path etc.).\n",
    "Dessa forma, foi então utilizado para cada atividade (funcionalidade ou grupo de funcionalidades) concluída, um teste unitário de auxílio, garantindo o funcionamento desta.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que são sementes e quais foram utilizadas\n",
    "O coletor precisa de uma origem de busca, ou seja, ele precisa partir de uma determinada página (comumente chamada de semente) para que ele seja capaz de encontrar outras novas páginas e conteúdos na Web. As novas páginas são encontradas  através dos endereços citados por meio de hiperlinks. Após a análise de cada semente, obtém-se então o conjunto de links que podem ser utilizados para acessar novas páginas (também chamadas de páginas vizinhas). Para a realização de testes e execução foram utilizadas as seguintes sementes:\n",
    "\n",
    "\n",
    "\n",
    "https://www.uol.com.br/, \n",
    "https://www.globo.com/, \n",
    "https://www.terra.com.br/, \n",
    "https://www.msn.com/pt-br\n",
    "https://www.techtudo.com.br/, \n",
    "https://canaltech.com.br/, \n",
    "https://github.com/, \n",
    "https://pt.stackoverflow.com/, \n",
    "https://pt.wix.com/, \n",
    "https://br.wordpress.com/, \n",
    "https://www.pathofexile.com/, \n",
    "https://www.reddit.com/, \n",
    "https://www.r7.com/, \n",
    "https://www.twitch.tv\n",
    "https://www.estadao.com.br/, \n",
    "https://www.ig.com.br/, \n",
    "https://www.portaldoholanda.com.br/, \n",
    "https://www.gazetadopovo.com.br/, \n",
    "https://www.naosalvo.com.br/, \n",
    "https://www.Facebook.com/, \n",
    "https://www.Baidu.com/, \n",
    "https://www.Wikipedia.org/, \n",
    "https://www.Yahoo.com/, \n",
    "https://www.Qq.com/, \n",
    "https://www.Taobao.com/, \n",
    "https://www.Twitter.com/, \n",
    "https://www.Amazon.com/, \n",
    "https://www.Google.co.jp/, \n",
    "https://www.Tmall.com/, \n",
    "https://www.Vk.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como foi construído o algoritmo\n",
    "Três classes foram implementadas para fazer o coletor: Domain, Scheduler e PageFetcher. O Domain é responsável por armazenar a url do domínio acessado além de armazenar o momento do último acesso a este, além do limite de tempo entre requisições a este domínio. Scheduler é responsável por armazenar as filas de URLs a serem requisitadas e a realizar o controle sobre elas. O PageFetcher serão as threads responsáveis por fazer as requisições das URLs obtidas por meio do escalonador (instância da classe Scheduler). As classes foram modularizadas de forma a serem implementadas e distribuídas dentre as atividades descritas a seguir:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise e Resultados\n",
    "**Configuração  \n",
    "Utilizadas 30 seeds  \n",
    "Limite de 1000 páginas  \n",
    "Profundidade limite de 6**  \n",
    "\n",
    "**10 Threads:** Tempo final de execução: 237.4529s  \n",
    "**30 Threads:** Tempo final de execução: 219.5009s  \n",
    "**50 Threads:** Tempo final de execução: 206.3777s  \n",
    "**70 Threads:** Tempo final de execução: 201.1997s  \n",
    "**90 Threads:** Tempo final de execução: 198.4257s  \n",
    "**110 Threads:** Tempo final de execução: 199.4689s  \n",
    "**130 Threads:** Tempo final de execução: 222.2311s  \n",
    "\n",
    "\n",
    "O comportamento do tempo de execução é esperado, a sua diminuição se torna irrisória a medida que o número de threads é aumentado, isso se dá pelos seguintes fatores:  \n",
    "- Domínios possuem um limite de tempo para um novo acesso, o que impede em alguns momentos requisições subsequentes que possam ultrapassariam o tempo mínimo.  \n",
    "- A divisão de trabalho cresce proporcionalmente à concorrência pelo processador, e a partir de determinado ponto, fica inviável o aumento do número de threads pois estas poderiam piorar o tempo de processamento.  \n",
    "- O número de threads do processador utilizado para os testes é oito, o que cria limitações físicas para a divisão de trabalho(paralelização).  \n",
    "\n",
    "Mesmo com essas limitações da melhoria de performance é possível perceber que o aumento do número de threads até certo ponto é benéfico para o coletor e deve ser utilizado com sabedoria embasada em estudos guiados por testes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit0ea829c2f13c4939aa5a8a9d699ecb85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
