{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centro Federal de Educação Tecnológica de Minas Gerais\n",
    "##### Recuperação de Informação\n",
    "## Coletor Vinigorbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor: Daniel Hasan  \n",
    "Alunos: Igor Lacerda Tomich e Vinícius França Lima Vilaça  \n",
    "Descritor do Coletor:  [https://crawlercefetmgvinigor.wordpress.com/](https://crawlercefetmgvinigor.wordpress.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução  \n",
    "\n",
    "\n",
    "A coleta de dados web, ou raspagem web, é uma forma de mineração que permite a extração de dados de sites da web convertendo-os em informação estruturada para posterior análise. Para a recuperação de informação são utilizadas aplicações que realizam a coleta de forma organizada, com definição clara sobre quais informações serão coletadas, assim como, de que forma as mesmas serão armazenadas para que o usuário possa utilizá-las. Essa aplicações de coleta de dados são comumente chamados de coletores ou, em inglês, crawlers. Este relatório irá discutir os principais desafios, as decisões de arquitetura e a implementação de um crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principais desafios\n",
    "O crawler é um algoritmo que pode ter uma estrutura paralelizável, nesta implementação a fizemos desta forma e portanto aproveitamos essa possibilidade de paralelização, assim, foi possível obter uma melhora na performance do algoritmo, isso significa que ele foi capaz de coletar uma mesma quantidade de página em menor tempo quando utilizado mais de uma thread. Apesar das vantagens da paralelização é necessário que se tome os devidos cuidados, pois na estrutura utilizada, as threads compartilham algumas variáveis e por isso foi um desafio mantê-las síncronas de modo que continuasse existindo condição de corrida, ou seja, nenhuma thread ficasse totalmente bloqueada e que também não ocorresse incongruência nos dados ou que erros inesperados ocorressem.\n",
    "\tAinda foi preciso que as funções ou “módulos” (atividades), fossem implementados da maneira mais correta possível, de forma que não existisse desvios da implementação requerida, isso significa que todos os testes unitário não puderam falhar.\n",
    "\tAmbos os integrantes do grupo também não possuíam muito contato com a linguagem de implementação Python e por isso tivemos um retardo na implementação com problemas relacionados a sintaxe.\n",
    "\tPor fim, ainda foi preciso realizar uma refatoração de uma das implementações(get_next_url)  de modo que esta não mais bloqueasse as outras threads de executarem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisões de arquitetura\n",
    "A linguagem utilizada foi o Python 3 aliado ao uso do Jupyter Notebook e a IDE Visual Studio Code. Para a execução do trabalho foram implementadas as atividades propostas no jupyter notebook através do código disponibilizado pelo professor da disciplina. Além disto algumas bibliotecas essenciais para o funcionamento do algoritmo foram utilizadas.  \n",
    "\n",
    "- Beautiful Soup:  Usado para a recuperação de dados em conteúdos HTML por meio de seu parser.  \n",
    "- Threads: Usado para a paralelização de métodos.  \n",
    "- Robotparser: Usado para checar informações relacionadas ao robots.txt de um Web Site, como por exemplo, se é possível realizar o fetch da página.  \n",
    "- Urlparse: Este módulo é usado para definir uma interface de acesso a uma URL, de modo que a URL seja quebrada ou remontada dentre seus componentes (addressing scheme, network location, path etc.).\n",
    "Dessa forma, foi então utilizado para cada atividade (funcionalidade ou grupo de funcionalidades) concluída, um teste unitário de auxílio, garantindo o funcionamento desta.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que são sementes e quais foram utilizadas\n",
    "O coletor precisa de uma origem de busca, ou seja, ele precisa partir de uma determinada página (comumente chamada de semente) para que ele seja capaz de encontrar outras novas páginas e conteúdos na Web. As novas páginas são encontradas  através dos endereços citados por meio de hiperlinks. Após a análise de cada semente, obtém-se então o conjunto de links que podem ser utilizados para acessar novas páginas (também chamadas de páginas vizinhas). Para a realização de testes e execução foram utilizadas as seguintes sementes:\n",
    "\n",
    "\n",
    "\n",
    "https://www.uol.com.br/, \n",
    "https://www.globo.com/, \n",
    "https://www.terra.com.br/, \n",
    "https://www.msn.com/pt-br\n",
    "https://www.techtudo.com.br/, \n",
    "https://canaltech.com.br/, \n",
    "https://github.com/, \n",
    "https://pt.stackoverflow.com/, \n",
    "https://pt.wix.com/, \n",
    "https://br.wordpress.com/, \n",
    "https://www.pathofexile.com/, \n",
    "https://www.reddit.com/, \n",
    "https://www.r7.com/, \n",
    "https://www.twitch.tv\n",
    "https://www.estadao.com.br/, \n",
    "https://www.ig.com.br/, \n",
    "https://www.portaldoholanda.com.br/, \n",
    "https://www.gazetadopovo.com.br/, \n",
    "https://www.naosalvo.com.br/, \n",
    "https://www.Facebook.com/, \n",
    "https://www.Baidu.com/, \n",
    "https://www.Wikipedia.org/, \n",
    "https://www.Yahoo.com/, \n",
    "https://www.Qq.com/, \n",
    "https://www.Taobao.com/, \n",
    "https://www.Twitter.com/, \n",
    "https://www.Amazon.com/, \n",
    "https://www.Google.co.jp/, \n",
    "https://www.Tmall.com/, \n",
    "https://www.Vk.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como foi construído o algoritmo\n",
    "Três classes foram implementadas para fazer o coletor: Domain, Scheduler e PageFetcher. O Domain é responsável por armazenar a url do domínio acessado além de armazenar o momento do último acesso a este, além do limite de tempo entre requisições a este domínio. Scheduler é responsável por armazenar as filas de URLs a serem requisitadas e a realizar o controle sobre elas. O PageFetcher serão as threads responsáveis por fazer as requisições das URLs obtidas por meio do escalonador (instância da classe Scheduler). As classes foram modularizadas de forma a serem implementadas e distribuídas dentre as atividades descritas a seguir:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 1\n",
    "Esta atividade exige a implementação dos métodos accessed_now que modifica o último acesso com a data/hora atual usando um objeto datetime, time_since_last_access que retorna um objeto TimeDelta com a diferença da data atual e a data do último acesso e is_accessible que verdadeiro se o domínio estiver acessível. Para o primeiro método apenas atribuiu-se ao atributo time_last_access a data atual . para o segundo método fez-se o retorno da data atual subtraído de time_last_access.  \n",
    "ref. accessed_now: https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L16  \n",
    "ref.time_since_last_access: https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L13  \n",
    "ref. is_accessible: https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L19  \n",
    "ref. time_last_access: https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L9  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 2\n",
    "A fila será implementada por um dicionário ordenado em que a chave será um objeto da classe domínio e o valor serão uma lista de URLs. Para que seja possível a busca do domínio, foi implementado os métodos __hash__ e __eq__ de forma que o acesso ao dicionário possa ser feito por um objeto Domain ou uma url do tipo String.  \n",
    "ref. __ hash __ https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L24  \n",
    "ref. __ eq __ https://github.com/viniciusfdev/collector/blob/master/crawler/domain.py#L27  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 3\n",
    "O método can_add_page irá retornar verdadeiro caso seja possível adicionar a página obj_url (objeto da classe ParseResult) que foi coletada na profundidade int_depth. Para que seja possível adicionar na lista, esta página não deve ter sido descoberta e nem com a profundidade maior que o limite. Este método será testado por teste unitário apenas após a implementação da adição e obtenção das URLs.\n",
    "ref. can_add_page https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L51  \n",
    "ref. obj_url https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L56  \n",
    "ref. ParseResult https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L2  \n",
    "ref. int_depth https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L56  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 4:\n",
    "O método implementado foi o add_new_page, responsável por adicionar uma nova página ao dicionário de urls a pesquisar. Verifica com o método can_add_page descrito na atividade acima se é possível adicionar uma nova url. Então se a url não está contida no dicionário url por domínio, cria-se uma nova posição no dicionário  para posteriormente adicionar nesta posição a tupla (url,profundidade). Caso já esteja contida, apenas adiciona a tupla na posição correspondente do dicionário, retornando verdadeiro. Caso a url não pode ser adicionada retorna falso.\n",
    "ref. add_new_page https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L62  \n",
    "ref. can_add_page https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L51  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 5:\n",
    "O método get_next_url  é responsável por pegar a próxima url na fila de urls. inicialmente retorna nulo se a lista  de domínios estiver vazia. Entra-se em um laço no qual para todo domínio no dicionário de url por domínio for acessível, verifica-se se o domínio não está contido no dicionaŕio. Caso esteja, adiciona-se em uma lista de domínios para serem removidos. Retorna então  a primeira URL do primeiro servidor que estiver acessível. Caso não seja acessívelretorna nulo.  \n",
    "ref. get_next_url https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L81  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 6:\n",
    "Foi implementado o método can_fetch_page, que é responsável por retornar verdadeiro caso a url passada como parâmetro pode ser coletada de acordo com o Robots.txt da mesma. Inicialmente verifica-se se a url não está contida no dicionário de robôs por domínio, para atribuir ao parser a nova url do arquivo Robot. Então atribui-se ao dicionaŕio robos por domínio a nova url tratada em netloc. Caso contrário retorna falso.\n",
    "ref. can_fetch_page https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L124  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 7:\n",
    "Esta atividade é responsável apenas em criar uma lista(arr_urls_seeds) e preenchê-la com urls seed definidas pelo grupo que posteriormente receberá como parâmetro no construtor da classe do escalonador(modificação do grupo). As Urls escolhidas estão descritas na seção “Decisões de Arquitetura”.  \n",
    "ref. arr_urls_seeds https://github.com/viniciusfdev/collector/blob/master/crawler/__init__.py#L40  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 8:\n",
    "Foi implementado o método request_url, que é responsável por fazer uma requisição à url  e retorna o conteúdo  em binário. O método cria uma instância da  classe ParseResult com a url target e logo posteriormente faz-se a requisição para retornar a resposta se for do tipo html caso contrário retorna nulo.  \n",
    "ref. request_url:  https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L13  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 9:\n",
    "Foi implementado o método discover_links, que é responsável em descobrir novas urls da requisição feita pelo método descrito na atividade 8(request_url). Para isso utilizou-se da biblioteca “BeautifulSoup” que faz um mapeamento do binário recebido como parâmetro que corresponde ao atributo bin_str_content. Inicialmente verifica-se a consistência do link definindo a sua profundidade para depois pegar a sua url em referência do hyperLink. Posteriormente é tratado caso a nova url não possua o http. Então o atributo obj_new_url recebe a url do método urlparser da biblioteca urllib.parse. Para finalizar verifica se o domínio não está contido na url, caso esteja incrementa em 1 a profundidade e então retorna a tupla do novo obj_new_url e a nova profundidade init_depth.  \n",
    "ref. discover_links: https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L32  \n",
    "ref. bin_str_content: https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L32  \n",
    "ref. obj_new_url: https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L32  \n",
    "ref. obj_new_url  https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L51  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 10:\n",
    "Foi implementado o método crawl_new_url, este método solicita ao escalonador uma nova URL por meio do método get_next_url, a url de retorno é avaliada e se acaso ela for nula o método aguarda pelo tempo mínimo entre requisições, caso não seja nula, esta é então enviada ao método request_url que retorna o html da url requisitada, em seguida o html é percorrido pelo método discover_links retornando todas as urls descobertas no html. Ao final as urls descobertas são armazenadas no escalonador para futuras coletas.  \n",
    "ref. crawl_new_url https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L62  \n",
    "ref. get_next_url https://github.com/viniciusfdev/collector/blob/master/crawler/scheduler.py#L81  \n",
    "ref. request_url https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L32  \n",
    "ref. discover_links https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L32  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 11:\n",
    "O escalador deve ser instanciado respetivamente com o nome do user agent sugerido pelo grupo, o limite de páginas coletadas, o limite de profundidade por página e a lista de seeds. Para a inicialização do processo são instanciadas o número de threads necessário ou avaliados pelo usuário, essas threads são inicializadas pelo método start da classe Thread que em seguida recebem a chamada ao método join, que realiza a espera neste ponto de chamada pela finalização da execução do método run das threads.  \n",
    "ref. scheduler https://github.com/viniciusfdev/collector/blob/master/crawler/__init__.py#L42  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atividade 12:\n",
    "Esta é a implementação do método run método é responsável por executar a coleta enquanto possuir  páginas a serem coletadas. Consiste em um laço no qual verifica-se se o escalador não terminou seus processos. Caso não tenha terminado, chama o método craw_new_url que coleta uma nova URL vinda do escalonador acrescido em 1 o número de páginas contadas pelo escalonador.  \n",
    "ref. run https://github.com/viniciusfdev/collector/blob/master/crawler/page_fetcher.py#L95  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise e Resultados\n",
    "**Configuração  \n",
    "Utilizadas 30 seeds  \n",
    "Limite de 1000 páginas  \n",
    "Profundidade limite de 6**  \n",
    "\n",
    "**10 Threads:** Tempo final de execução: 237.4529s  \n",
    "**30 Threads:** Tempo final de execução: 219.5009s  \n",
    "**50 Threads:** Tempo final de execução: 206.3777s  \n",
    "**70 Threads:** Tempo final de execução: 201.1997s  \n",
    "**90 Threads:** Tempo final de execução: 198.4257s  \n",
    "**110 Threads:** Tempo final de execução: 199.4689s  \n",
    "**130 Threads:** Tempo final de execução: 222.2311s  \n",
    "\n",
    "\n",
    "O comportamento do tempo de execução é esperado, a sua diminuição se torna irrisória a medida que o número de threads é aumentado, isso se dá pelos seguintes fatores:  \n",
    "- Domínios possuem um limite de tempo para um novo acesso, o que impede em alguns momentos requisições subsequentes que possam ultrapassariam o tempo mínimo.  \n",
    "- A divisão de trabalho cresce proporcionalmente à concorrência pelo processador, e a partir de determinado ponto, fica inviável o aumento do número de threads pois estas poderiam piorar o tempo de processamento.  \n",
    "- O número de threads do processador utilizado para os testes é oito, o que cria limitações físicas para a divisão de trabalho(paralelização).  \n",
    "\n",
    "Mesmo com essas limitações da melhoria de performance é possível perceber que o aumento do número de threads até certo ponto é benéfico para o coletor e deve ser utilizado com sabedoria embasada em estudos guiados por testes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerações Finais\n",
    "Este trabalho tem a intenção de compreender o funcionamento de um coletor. Uma única thread pode deixá-lo relativamente limitado e extremamente lento. Para resolver este problema implementou-se um coletor paralelizável em que cada thread seja capaz de extrair do robot.txt de cada página seed definida pelo grupo novas páginas para que seja possível descobrir cada vez mais novas urls de diversas profundidades e assim ampliar a coleta. É preciso que durante as coletas o coletor evite ao máximo sobrecarregar os servidores web dos sites coletados, dar intervalos entre as requisições e acessar apenas o que é público. O coletor também deve somente coletar os dados destinados a coletores que são os metadados contidos no arquivo “robot.txt” como por exemplo tem-se os campos: User-Agent, Disallow, Allow, Craw-dela. Para que não exista problemas com terceiros é ainda necessário seguir as boas maneiras e éticas na coleta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit0ea829c2f13c4939aa5a8a9d699ecb85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
